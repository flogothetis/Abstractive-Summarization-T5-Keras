{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AbstractiveSummarizationT5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1IM-eu-l0elVVEbu4F7KtQ2ynoMogLU1R",
      "authorship_tag": "ABX9TyNiSt5vFdxWpOL9S3IsehS0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "566003fed25e45b2a30684e4f8c40094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_21bff9d9185b4879a607950346392524",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fa5a6a3c381244e191d65b33a0dcf96f",
              "IPY_MODEL_7500a0948b0543e9bf558dd7e7eeae55"
            ]
          }
        },
        "21bff9d9185b4879a607950346392524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa5a6a3c381244e191d65b33a0dcf96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c9214633945940aa8045fbfd46e6ea33",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e98303c9bf0c424bbabb4f927257a79b"
          }
        },
        "7500a0948b0543e9bf558dd7e7eeae55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e083d296b9914b3fa76eb9f67ed9622a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 792k/792k [00:06&lt;00:00, 118kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b96b3dbc0cf40abb4833730836c83e4"
          }
        },
        "c9214633945940aa8045fbfd46e6ea33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e98303c9bf0c424bbabb4f927257a79b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e083d296b9914b3fa76eb9f67ed9622a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b96b3dbc0cf40abb4833730836c83e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYZikQP-PTST",
        "outputId": "8a1c5141-2870-4cc7-825f-faab14eed60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 18.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a61b7d3caba698558dc8778a1888db3dcff718a3a121c69826aec517b84c90c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjH-VWUQwnBS"
      },
      "source": [
        "## T5 Tokenizer and Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r6qg7iZwllM",
        "outputId": "041f58f4-8f10-4b73-93c1-d6a3839b9559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "566003fed25e45b2a30684e4f8c40094",
            "21bff9d9185b4879a607950346392524",
            "fa5a6a3c381244e191d65b33a0dcf96f",
            "7500a0948b0543e9bf558dd7e7eeae55",
            "c9214633945940aa8045fbfd46e6ea33",
            "e98303c9bf0c424bbabb4f927257a79b",
            "e083d296b9914b3fa76eb9f67ed9622a",
            "4b96b3dbc0cf40abb4833730836c83e4"
          ]
        }
      },
      "source": [
        "# Bert Tokenizer (leverages SentencePiece and Unicode Normalizaiton)\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "# Start of sentence token\n",
        "end_token = tokenizer.eos_token\n",
        "# End of sentence token\n",
        "start_token = tokenizer.pad_token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "566003fed25e45b2a30684e4f8c40094",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ekCFXqG6hBT"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkdyLNbtPf1z",
        "outputId": "08f93893-3172-47c7-e28b-d81e21b3c6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Load Data from GDrive\n",
        "news = pd.read_excel(\"/content/drive/My Drive/news.xlsx\")\n",
        "news.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)\n",
        "print(news.head(100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             Headline                                              Short\n",
            "0   4 ex-bank officials booked for cheating bank o...  The CBI on Saturday booked four former officia...\n",
            "1      Supreme Court to go paperless in 6 months: CJI  Chief Justice JS Khehar has said the Supreme C...\n",
            "2   At least 3 killed, 30 injured in blast in Sylh...  At least three people were killed, including a...\n",
            "3   Why has Reliance been barred from trading in f...  Mukesh Ambani-led Reliance Industries (RIL) wa...\n",
            "4   Was stopped from entering my own studio at Tim...  TV news anchor Arnab Goswami has said he was t...\n",
            "..                                                ...                                                ...\n",
            "95  Houseboat owners should not pollute Dal lake: ...  Terming the Dal Lake as a &#34;treasure&#34; f...\n",
            "96  Will lions in UP zoos have to live on palak pa...  Speaking about the issue of meat shortage in U...\n",
            "97  Delhi MCD elections a chance to uproot AAP gov...  Addressing party workers ahead of the MCD elec...\n",
            "98  UP Minister urges rich Muslims to give up Haj ...  Uttar Pradesh&#39;s Muslim Waqf and Haj Minist...\n",
            "99  Islamic leader in Canada asks Muslims to kill ...  In a YouTube video, an Imam in Canada has repo...\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnQsNl-l9jsc"
      },
      "source": [
        "## Creating Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d435dqf53M00"
      },
      "source": [
        "# Clean Text\n",
        "def preprocessText(text):\n",
        "    #remove content into parenthesis\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "    #remove quotes \n",
        "    text= re.sub('\"','', text)\n",
        "    #delete whitespaces\n",
        "    text =  \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVQJk47dUbkR"
      },
      "source": [
        "max_len_doc = -1\n",
        "max_len_sum = -1\n",
        "\n",
        "# Keep all training inputs into a dictionary.\n",
        "train_data = {\n",
        "    'input_ids': [],\n",
        "    'attention_mask':[],\n",
        "    'decoder_inputs_ids':[],\n",
        "    \"decoder_attention_mask\":[]\n",
        "}\n",
        "# Preprocess and Tokenize\n",
        "for i in news['Short']:\n",
        "    # Clean text \n",
        "    trainText = preprocessText(i)\n",
        "    # From text to tensor\n",
        "    ids= tokenizer.encode_plus(trainText)\n",
        "    # Find longest text\n",
        "    if (max_len_doc < len(ids['input_ids'])):\n",
        "        max_len_doc = len(ids['input_ids'])\n",
        "    # Get input tokens \n",
        "    train_data['input_ids'].append(ids['input_ids'])\n",
        "    # Get attention mask\n",
        "    train_data['attention_mask'].append(ids['attention_mask'])\n",
        "\n",
        "for i in news['Headline']:\n",
        "    # Add start token <pad> in front of summary\n",
        "    labelsText= start_token+ \" \"+ preprocessText(i)\n",
        "    decoder_ids = tokenizer.encode_plus(labelsText)\n",
        "    if(max_len_sum < len(decoder_ids['input_ids'])):\n",
        "        max_len_sum = len(decoder_ids['input_ids'])\n",
        "    train_data['decoder_inputs_ids'].append(decoder_ids['input_ids'])\n",
        "    train_data['decoder_attention_mask'].append(decoder_ids['attention_mask'])   \n",
        "#Convert to array of lists \n",
        "for key in train_data:\n",
        "    train_data[key]= np.array(train_data[key])   \n",
        "#Pad sequence to max len   \n",
        "train_data['input_ids'] = tf.keras.preprocessing.sequence.pad_sequences(train_data['input_ids'], maxlen=\n",
        "                                              max_len_doc, padding= 'post', truncating='post' )\n",
        "train_data['attention_mask'] = tf.keras.preprocessing.sequence.pad_sequences(train_data['attention_mask'], maxlen=\n",
        "                                              max_len_doc, padding= 'post', truncating='post')\n",
        "train_data['decoder_inputs_ids'] = tf.keras.preprocessing.sequence.pad_sequences(train_data['decoder_inputs_ids'], maxlen=\n",
        "                                              max_len_sum, padding= 'post', truncating='post')\n",
        "train_data['decoder_attention_mask'] = tf.keras.preprocessing.sequence.pad_sequences(train_data['decoder_attention_mask'], maxlen=\n",
        "                                              max_len_sum, padding= 'post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNwdtKmvP9p2"
      },
      "source": [
        "print (\"Number of training examples: \", len(train_data['input_ids'])\n",
        "print (\"Max length of tokens of main text:\" max_len_doc)\n",
        "print (\"Max length of token of sumamry: \", max_len_sum)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuzHl1ByzPGU"
      },
      "source": [
        "## Create Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpoJgOoFenYO"
      },
      "source": [
        "def createModel ():\n",
        "    # T5 Model - Hugging Face\n",
        "    T5 = TFT5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "    task_specific_params = T5.config.task_specific_params\n",
        "    if task_specific_params is not None:\n",
        "        T5.config.update(task_specific_params.get(\"summarization\", {}))\n",
        "    # Inputs\n",
        "    input_ids = tf.keras.Input(shape=(max_len_doc,),dtype=tf.int32)\n",
        "    decoder_inputs_ids = tf.keras.Input(shape=(max_len_sum-1,),dtype=tf.int32)\n",
        "    attention_mask = tf.keras.Input(shape=(max_len_doc,),dtype=tf.int32)\n",
        "    decoder_attention_mask = tf.keras.Input(shape=(max_len_sum-1,),dtype=tf.int32)\n",
        "    # Get T5 output\n",
        "    logits = T5(input_ids, attention_mask = attention_mask, \n",
        "                decoder_input_ids=decoder_inputs_ids,\n",
        "                decoder_attention_mask= decoder_attention_mask)[0]\n",
        "    # return Keras model            \n",
        "    return tf.keras.Model(inputs= [input_ids, attention_mask, \n",
        "                                   decoder_inputs_ids,decoder_attention_mask],\n",
        "                           outputs=logits)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4d0mN6_H_M"
      },
      "source": [
        "## HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f6EccPk_HPb"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS  = 4\n",
        "LEARNING_RATE = 3e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L20qnQG7f6wZ"
      },
      "source": [
        "## Use TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1LRRj6wfyJp",
        "outputId": "d49d43c2-c807-46be-82c7-46d1478926af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "use_tpu = True\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = createModel()\n",
        "else:\n",
        "    model = createModel()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.25.112.194:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.25.112.194:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.25.112.194:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.25.112.194:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "All model checkpoint weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_t5for_conditional_generation_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "shared (TFSharedEmbeddings)  multiple                  16449536  \n",
            "_________________________________________________________________\n",
            "encoder (TFT5MainLayer)      multiple                  18881280  \n",
            "_________________________________________________________________\n",
            "decoder (TFT5MainLayer)      multiple                  25176064  \n",
            "=================================================================\n",
            "Total params: 60,506,880\n",
            "Trainable params: 60,506,880\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_17 (InputLayer)           [(None, 218)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_19 (InputLayer)           [(None, 218)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_t5for_conditional_generation ((None, 50, 32128),  60506880    input_17[0][0]                   \n",
            "                                                                 input_19[0][0]                   \n",
            "                                                                 input_20[0][0]                   \n",
            "                                                                 input_18[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 60,506,880\n",
            "Trainable params: 60,506,880\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m8dTVW--qFj"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYzw41RCcUIm",
        "outputId": "889630ba-6a51-4bed-f466-37f73697eefd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "adam_opt = tf.optimizers.Adam (learning_rate= LEARNING_RATE)\n",
        "model.compile(adam_opt, loss= loss, metrics=[loss])\n",
        "model.fit(x=[train_data['input_ids'],\n",
        "             train_data['attention_mask'],\n",
        "             train_data['decoder_inputs_ids'][:,:-1],\n",
        "             train_data['decoder_attention_mask'][:,:-1]],\n",
        "          y= train_data['decoder_inputs_ids'][:,1:],\n",
        "          batch_size= BATCH_SIZE, \n",
        "          epochs=EPOCHS,\n",
        "          verbose=2)\n",
        "          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0108s vs `on_train_batch_end` time: 0.0751s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0108s vs `on_train_batch_end` time: 0.0751s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "861/861 - 82s - loss: 1.0779 - sparse_categorical_crossentropy: 1.0779\n",
            "Epoch 2/4\n",
            "861/861 - 67s - loss: 0.6167 - sparse_categorical_crossentropy: 0.6167\n",
            "Epoch 3/4\n",
            "861/861 - 67s - loss: 0.5675 - sparse_categorical_crossentropy: 0.5675\n",
            "Epoch 4/4\n",
            "861/861 - 67s - loss: 0.5373 - sparse_categorical_crossentropy: 0.5373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8ddb3e2e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bJ3GaGu9Jvf"
      },
      "source": [
        "\n",
        "## Test Model using Greedy Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUiXcUF3KOxx"
      },
      "source": [
        "def getSummary(text):\n",
        "    # Start summary with <pad> token\n",
        "    summary = start_token\n",
        "    # Preprocess text\n",
        "    text = preprocessText(text)\n",
        "    # Convert text to tensor\n",
        "    ids = tokenizer.encode_plus(text)\n",
        "    input_ids = ids['input_ids']\n",
        "    attention_mask = ids['attention_mask']\n",
        "    # Pad text sequence\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences([input_ids], maxlen=\n",
        "                                                max_len_doc, padding= 'post', truncating='post' )\n",
        "    attention_mask = tf.keras.preprocessing.sequence.pad_sequences([attention_mask], maxlen=\n",
        "                                                max_len_doc, padding= 'post', truncating='post')\n",
        "    counter = 1 \n",
        "    prev_summary =\"\"\n",
        "    while (counter < max_len_sum and len(prev_summary)!=len(summary)):\n",
        "        # Convert summary to \n",
        "        decoder_ids = tokenizer.encode_plus(summary)\n",
        "        decoder_input_ids = decoder_ids['input_ids']\n",
        "        decoder_attention_mask = decoder_ids['attention_mask']\n",
        "        #Pad sequence to max len   \n",
        "        decoder_inputs_ids = tf.keras.preprocessing.sequence.pad_sequences([decoder_input_ids[:-1]], maxlen=\n",
        "                                                max_len_sum, padding= 'post', truncating='post')\n",
        "        decoder_attention_mask = tf.keras.preprocessing.sequence.pad_sequences([decoder_attention_mask[:-1]], maxlen=\n",
        "                                                max_len_sum, padding= 'post', truncating='post')\n",
        "        # Decoder of T5 predicts the next word\n",
        "        pred = model.predict([input_ids,attention_mask, decoder_inputs_ids, decoder_attention_mask])\n",
        "        # Decode text \n",
        "        new_summary = tokenizer.decode(np.argmax(pred, axis=-1)[0,:counter])\n",
        "        prev_summary = summary\n",
        "        # Get new summary and prepare it for the next prediction\n",
        "        summary = start_token +\" \"+ new_summary\n",
        "        counter+=1\n",
        "    #remove <pad> token\n",
        "    return re.sub(r'<pad>',\"\",summary)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7rrS8SKmYCI",
        "outputId": "9b943432-2609-4689-8cf4-692e608f15e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "getSummary(\"With your permission we and our partners may use precise geolocation\\\n",
        "  data and identification through device scanning. You may click to consent to our\\\n",
        "  and our partners’ processing as described above. Alternatively you may access more\\\n",
        "   detailed information and change your preferences before consenting or to refuse consenting.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' We may use precise geolocation data through device scanning'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJZyLOZsmbO3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}