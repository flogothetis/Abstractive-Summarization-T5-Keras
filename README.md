# Abstractive-Summarization-T5-Keras

This abstractive text summarization is one of the most challenging tasks in natural language processing, involving understanding of long passages, information compression, and
language generation. The dominant paradigm for training machine learning models to do this is sequence-to-sequence (seq2seq) learning, where a neural network learns to map
input sequences to output sequences. While these seq2seq models were initially developed using recurrent neural networks, Transformer encoder-decoder (T5) models have recently
become favored as they are more effective at modeling the dependencies present in the long sequences encountered in summarization.


